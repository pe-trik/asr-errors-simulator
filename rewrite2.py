import json
from numpy.random import choice, random_sample, randint, seed
import tqdm
from pathos.multiprocessing import ProcessPool

from mosestokenizer import MosesTokenizer
import string
import sys

CORPUS_INFO_WORD = "<CORPUS_INFO_WORD>"

def load_rules(rules_path, goal_wer):
    rules = {original: forms for line in open(rules_path).readlines() for original, forms in json.loads(line).items()}
    unk = rules[CORPUS_INFO_WORD]
    a = (unk['p_insert'] + unk['p_delete']) * 1*unk['p_substitute']
    b = -(unk['p_insert'] + unk['p_delete'] + 1*unk['p_substitute'])
    c = goal_wer
    D = b * b - 4 * a * c
    c1 = (-b + D ** 0.5) / (2 * a)
    c2 = (-b - D ** 0.5) / (2 * a)
    c = min(c1, c2) if min(c1, c2) > 0 else max(c1, c2)
    print('c1:',c1,'c2:', c2, 'c:',c ,file=sys.stderr)
    print(unk,file=sys.stderr)
    unk['p_insert'] *= c
    unk['p_delete'] *= c
    unk['p_substitute'] *= 1*c
    unk['p_transmit'] = (1 - unk['p_insert'] - unk['p_delete']) / (1 - unk['p_insert'])
    unk['p_insert'] = unk['p_insert'] / (1 + unk['p_insert'])
    print(unk,file=sys.stderr)
    print('wer:',unk['p_insert'] / (1 - unk['p_insert']) + unk['p_delete'] + (1-unk['p_insert'] - unk['p_delete'])*unk['p_substitute'])
    vocab = set()
    for forms in rules.values():
        vocab.update(forms.keys())
    rules['<vocab>'] = list(vocab)
    return rules

def simple_rewrite(line):
    o = ''
    for w in line.lower().strip().split():
        while random_sample() < unk['p_insert']:
            o += choice(list(r[''].keys()), 1, p=list(r[''].values()))[0] + ' '
        if random_sample() < unk['p_transmit']:
            if random_sample() < unk['p_substitute']:
                if w not in r.keys() or len(r[w]) == 0:
                    idx = randint(0, len(vocab))
                    o += vocab[idx] + ' '
                else:
                    o += choice(list(r[w].keys()), 1, p=list(r[w].values()))[0] + ' '
            else:
                o += w + ' '
    return o.strip()

def ispunctuation(x):
    return all(i in string.punctuation for i in x)

def punct_rewrite(line, tokenizer, punct_option="", casing_option="", full_stop=False, cap_start=False, tag_gaps=False):
    o = []
    tokens = tokenizer.tokenize(line)

    for w in tokens:
        if ispunctuation(w):
            if punct_option == "keep":
                o.append(w)
                continue
            elif punct_option == "no":
                continue
        w_orig = w
        while random_sample() < unk['p_insert']: # insertion
            o.append(choice(list(r[''].keys()), 1, p=list(r[''].values()))[0])
        if random_sample() < unk['p_transmit']:
            if random_sample() < unk['p_substitute']:  # substitution
                w = w.lower()
                if w not in r.keys() or len(r[w]) == 0:
                    idx = randint(0, len(vocab))
                    n = vocab[idx]
                else:
                    n = choice(list(r[w].keys()), 1, p=list(r[w].values()))[0]
                if casing_option == "keep" and w_orig[0].isupper():
                    n = n[0].upper() + n[1:]
            else:  # transmission
                # keep casing, later it is lowercased if it is also an option
                n = w_orig
            if n is not None:
                o.append(n)
        elif tag_gaps:  # deletion
            o.append("<gap:%d>" % len(w))

    detok = tokenizer.detokenize(o)
    if full_stop:
        if detok and detok[-1] not in ".!?":
            detok += "."
    if casing_option == "lower":
        detok = detok.lower()
    if cap_start and detok:
        detok = detok[0].upper() + detok[1:]
    return detok



def check_list_options(arg, options):
    o = options.split(" ")
    if arg not in o:
        return False
    return True


if __name__ == '__main__':

    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('rules', type=str)
    parser.add_argument('goal_wer', type=float, help="Goal WER, a float between 0.0 and 1.0.")
    parser.add_argument('data', type=str,help="Input file. If -, use stdin.")
    parser.add_argument('output', type=str)

    PUNCT_OPTIONS = "keep random no"
    parser.add_argument('--punct', type=str,help=f"Punctuation option: {PUNCT_OPTIONS}. "
        "Keep: the punctuation is kept from the input sentence. "
        "Random: punctuation tokens are treated as normal tokens, e.g. as OOV if not in rules, and randomly transmitted/substituted/inserted/deleted. "
        "No: all punctuation is retrieved from input. It can appear in the output only if it is a part of non-punct-only token, or generated by a rule.",
        default="random"
        )

    CASING_OPTIONS = "keep lower"
    parser.add_argument('--casing', type=str,help=f"Output casing option: {CASING_OPTIONS}. "
            "Keep: keep casing from the input in transmission and substitution. Lower: make everything lowercase."
            "Note that it is assumed that the rules are all in lowercase, so each token is lowercased before searching the rule for it.",
            default="keep")
    parser.add_argument('--cap-start',action='store_true',default=False,help="Capitalize the first character of each output line.")
    parser.add_argument('--full-stop',action='store_true',default=False,help="Full-stop: make sure that every line of input is terminated by one punctuation mark, \".\" by default, or \"!\", \"?\" or other if keep or random generates it.")

    parser.add_argument('--tag-gaps',action='store_true',default=False,help="Insert tag for every deleted word. The tag has a form e.g. '<gap:N>', where N is a number of characters.")


    parser.add_argument('--lang', type=str,help=f"Language option for MosesTokenizer. Default is \"en\".",default="en")

    parser.add_argument('--seed', type=int,nargs="?",help=f"Random seed. If this option is unused, the seed is not set. No argument: the seed is 1234.", default=None)

    args = parser.parse_args()

    if "--seed" in sys.argv:
        if args.seed is None:
            seed_opt = 1234
        else:
            seed_opt = args.seed
        print("Setting the seed to",seed_opt,file=sys.stderr)
        seed(seed_opt)


    print(args)

    if not check_list_options(args.punct, PUNCT_OPTIONS):
        print("Unknown punctuation option. Choose one or more from",PUNCT_OPTIONS,file=sys.stderr)
        sys.exit(1)

    if not check_list_options(args.casing, CASING_OPTIONS):
        print("Unknown casing option. Choose one or more from",CASING_OPTIONS,file=sys.stderr)
        sys.exit(1)

    r = load_rules(args.rules, args.goal_wer)

    unk = r[CORPUS_INFO_WORD]
    vocab = r['<vocab>']

    tokenizer = MosesTokenizer(args.lang)

    if args.data == "-":
        f = sys.stdin
    else:
        f = open(args.data,"r")

    pool = ProcessPool()
    with open(args.output, 'w') as output:
        rew = lambda l: punct_rewrite(l, tokenizer, punct_option=args.punct, casing_option=args.casing, cap_start=args.cap_start, full_stop=args.full_stop, tag_gaps=args.tag_gaps)
        for r in tqdm.tqdm(pool.imap(rew, f)):
            print(r,file=output)

    if args.data != "-":
        f.close()


